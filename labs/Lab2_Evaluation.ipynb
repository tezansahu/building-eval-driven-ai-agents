{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Evaluating and Improving the Campus Event Agent"
      ],
      "metadata": {
        "id": "USEvxG6Sigo7"
      },
      "id": "USEvxG6Sigo7"
    },
    {
      "cell_type": "markdown",
      "id": "1d2dc188",
      "metadata": {
        "id": "1d2dc188"
      },
      "source": [
        "**Objective**: Systematically evaluate and improve agent performance using Azure AI Evaluation SDK\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Create structured test datasets with expected outcomes\n",
        "- Use Azure AI evaluators (Relevance, Task Adherence)\n",
        "- Build custom code-based evaluators (Conciseness)\n",
        "- Convert agent responses to trace format\n",
        "- Run baseline evaluations\n",
        "- Improve agents based on metrics\n",
        "- Measure improvement quantitatively\n",
        "\n",
        "## Key Concept: Evaluation-Driven Development\n",
        "\n",
        "Just like Test-Driven Development (TDD):\n",
        "1. **Create tests** (evaluation dataset)\n",
        "2. **Measure baseline** (current performance)\n",
        "3. **Improve** (fix identified issues)\n",
        "4. **Re-measure** (verify improvement)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A: Setup & Rebuild Agent"
      ],
      "metadata": {
        "id": "bQHcZXGVinM7"
      },
      "id": "bQHcZXGVinM7"
    },
    {
      "cell_type": "markdown",
      "id": "6730c1ad",
      "metadata": {
        "id": "6730c1ad"
      },
      "source": [
        "We'll rebuild a baseline version of the agent from Lab 1 with intentional issues."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all Microsoft Agents Framework\n",
        "!pip install -q agent-framework --pre"
      ],
      "metadata": {
        "id": "NJOUIo0girKn"
      },
      "id": "NJOUIo0girKn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ea3d15",
      "metadata": {
        "id": "88ea3d15"
      },
      "outputs": [],
      "source": [
        "# Install Azure AI Evaluation SDK\n",
        "!pip install -q azure-ai-evaluation matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e9f059",
      "metadata": {
        "id": "86e9f059"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from google.colab import userdata\n",
        "from agent_framework import ChatAgent\n",
        "from agent_framework.openai import OpenAIChatClient\n",
        "from typing import Annotated, List, Dict, Any\n",
        "from pydantic import Field\n",
        "import requests\n",
        "import sys\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "sys.path.append('/content')\n",
        "\n",
        "print(\"âœ… Libraries imported\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download utils.py helper functions\n",
        "!wget -q https://raw.githubusercontent.com/tezansahu/building-eval-driven-ai-agents/main/labs/utils.py -O utils.py\n",
        "\n",
        "from utils import print_agent_response, function_to_tool_schema, convert_agent_response_to_trace\n"
      ],
      "metadata": {
        "id": "zpuIlBW1jLTQ"
      },
      "id": "zpuIlBW1jLTQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fe308b",
      "metadata": {
        "id": "14fe308b"
      },
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "GITHUB_PAT = userdata.get('GITHUB_PAT')\n",
        "BACKEND_URL = \"\"  # TODO: Paste your ngrok URL from Lab 0\n",
        "\n",
        "if not BACKEND_URL:\n",
        "    print(\"âš ï¸ Set BACKEND_URL!\")\n",
        "else:\n",
        "    print(\"âœ… Configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Baseline Agent (Intentionally Has Issues)"
      ],
      "metadata": {
        "id": "u95d_Tocjcwk"
      },
      "id": "u95d_Tocjcwk"
    },
    {
      "cell_type": "markdown",
      "id": "8931c30f",
      "metadata": {
        "id": "8931c30f"
      },
      "source": [
        "This agent has problems we'll identify through evaluation:\n",
        "- Tool responses are too brief (no event details, no emojis)\n",
        "- Instructions are minimal\n",
        "- No conciseness guidelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb7a8ab",
      "metadata": {
        "id": "fdb7a8ab"
      },
      "outputs": [],
      "source": [
        "# Baseline tools (brief responses - intentionally lacking detail)\n",
        "\n",
        "def get_events() -> str:\n",
        "    \"\"\"Retrieve all campus events.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{BACKEND_URL}/events\")\n",
        "        events = response.json()\n",
        "        return f\"Found {len(events)} events: {','.join([event['name'] for event in events])}\" if events else \"No events\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_event_details(\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")]\n",
        ") -> str:\n",
        "    \"\"\"Get event details.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{BACKEND_URL}/events/{event_id}\")\n",
        "        if response.status_code == 404:\n",
        "            return \"Event not found\"\n",
        "        event = response.json()\n",
        "        return f\"{event['name']} on {event['date']}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "J3cCAkEOkM35"
      },
      "id": "J3cCAkEOkM35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def register_for_event(\n",
        "    student_id: Annotated[str, Field(description=\"Student ID\")],\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")],\n",
        "    student_name: Annotated[str, Field(description=\"Student name\")]\n",
        ") -> str:\n",
        "    \"\"\"Register student for event.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{BACKEND_URL}/events/{event_id}/register\",\n",
        "            json={\"student_id\": student_id, \"student_name\": student_name}\n",
        "        )\n",
        "        result = response.json()\n",
        "        return \"Registered\" if result.get(\"success\") else f\"Failed: {result.get('message')}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "7ktDwFPlkPFB"
      },
      "id": "7ktDwFPlkPFB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_event_participants(\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")]\n",
        ") -> str:\n",
        "    \"\"\"Get event participants.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{BACKEND_URL}/events/{event_id}/participants\")\n",
        "        if response.status_code == 404:\n",
        "            return \"Event not found\"\n",
        "        data = response.json()\n",
        "        count = data.get('participant_count', 0)\n",
        "        return f\"{count} participants\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "P3LYSuFbkTLp"
      },
      "id": "P3LYSuFbkTLp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASELINE_INSTRUCTIONS = \"You are a campus event assistant. Help students with events.\""
      ],
      "metadata": {
        "id": "ms6ixNSYkWr9"
      },
      "id": "ms6ixNSYkWr9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_client = OpenAIChatClient(\n",
        "    model_id=\"gpt-4o-mini\",\n",
        "    api_key=GITHUB_PAT,\n",
        "    base_url=\"https://models.github.ai/inference\"\n",
        ")"
      ],
      "metadata": {
        "id": "YJGIqkF-kXqR"
      },
      "id": "YJGIqkF-kXqR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b96439f",
      "metadata": {
        "id": "0b96439f"
      },
      "outputs": [],
      "source": [
        "# Baseline agent (minimal instructions)\n",
        "baseline_agent = ChatAgent(\n",
        "    chat_client=chat_client,\n",
        "    instructions=BASELINE_INSTRUCTIONS,\n",
        "    tools=[get_events, get_event_details, register_for_event, get_event_participants]\n",
        ")\n",
        "\n",
        "print(\"âœ… Baseline agent created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d969ee",
      "metadata": {
        "id": "83d969ee"
      },
      "outputs": [],
      "source": [
        "# Quick test\n",
        "test_response = await baseline_agent.run(\n",
        "    \"What events are happening on campus?\"\n",
        ")\n",
        "print_agent_response(test_response, show_details=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B: Create Test Dataset"
      ],
      "metadata": {
        "id": "Ofqn_PmAlLb_"
      },
      "id": "Ofqn_PmAlLb_"
    },
    {
      "cell_type": "markdown",
      "id": "bfbd79a0",
      "metadata": {
        "id": "bfbd79a0"
      },
      "source": [
        "A good test dataset includes:\n",
        "- Diverse query types\n",
        "- Expected tools to be called\n",
        "- Expected outcomes\n",
        "- Coverage of all capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7d4ea8",
      "metadata": {
        "id": "9c7d4ea8"
      },
      "outputs": [],
      "source": [
        "# For this walkthrough, we'll use 5 diverse queries covering key capabilities\n",
        "# Later, you can uncomment more test cases to expand your evaluation coverage\n",
        "\n",
        "test_dataset = [\n",
        "    {\n",
        "        \"query\": \"What events are happening on campus?\",\n",
        "        \"expected_tool\": \"get_events\",\n",
        "        \"expected_outcome\": \"Should call get_events to list all available events\",\n",
        "        \"category\": \"browse\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"query\": \"Show me all the upcoming events\",\n",
        "    #     \"expected_tool\": \"get_events\",\n",
        "    #     \"expected_outcome\": \"Should browse events and present formatted list\",\n",
        "    #     \"category\": \"browse\"\n",
        "    # },\n",
        "    {\n",
        "        \"query\": \"Tell me more about the AI Workshop\",\n",
        "        \"expected_tool\": \"get_event_details\",\n",
        "        \"expected_outcome\": \"Should get detailed information about specific event\",\n",
        "        \"category\": \"details\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"query\": \"What's TechFest 2024 about?\",\n",
        "    #     \"expected_tool\": \"get_event_details\",\n",
        "    #     \"expected_outcome\": \"Should retrieve event description, date, venue, capacity\",\n",
        "    #     \"category\": \"details\"\n",
        "    # },\n",
        "    {\n",
        "        \"query\": \"Register me for the AI Workshop. My name is Rahul and ID is S001\",\n",
        "        \"expected_tool\": \"register_for_event\",\n",
        "        \"expected_outcome\": \"Should register student with confirmation details\",\n",
        "        \"category\": \"registration\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"query\": \"I want to sign up for TechFest 2024. I'm Priya, student ID S002\",\n",
        "    #     \"expected_tool\": \"register_for_event\",\n",
        "    #     \"expected_outcome\": \"Should extract name, ID and register for event\",\n",
        "    #     \"category\": \"registration\"\n",
        "    # },\n",
        "    {\n",
        "        \"query\": \"Who's registered for the AI Workshop?\",\n",
        "        \"expected_tool\": \"get_event_participants\",\n",
        "        \"expected_outcome\": \"Should list all registered participants\",\n",
        "        \"category\": \"participants\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"query\": \"How many people signed up for TechFest?\",\n",
        "    #     \"expected_tool\": \"get_event_participants\",\n",
        "    #     \"expected_outcome\": \"Should get participant count and list\",\n",
        "    #     \"category\": \"participants\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"query\": \"Show me events, then tell me about the AI Workshop, and register me as Amit (S003)\",\n",
        "    #     \"expected_tool\": \"get_events\",\n",
        "    #     \"expected_outcome\": \"Should execute multi-step workflow: browse â†’ details â†’ register\",\n",
        "    #     \"category\": \"multi-step\"\n",
        "    # },\n",
        "    {\n",
        "        \"query\": \"What can you help me with?\",\n",
        "        \"expected_tool\": None,\n",
        "        \"expected_outcome\": \"Should respond without calling tools, explain capabilities\",\n",
        "        \"category\": \"information\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"âœ… Test dataset: {len(test_dataset)} cases\")\n",
        "for cat in set(t['category'] for t in test_dataset):\n",
        "    count = sum(1 for t in test_dataset if t['category'] == cat)\n",
        "    print(f\"   - {cat}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part C: Build Evaluators"
      ],
      "metadata": {
        "id": "KcgGwaorlN3J"
      },
      "id": "KcgGwaorlN3J"
    },
    {
      "cell_type": "markdown",
      "id": "af28c622",
      "metadata": {
        "id": "af28c622"
      },
      "source": [
        "We'll use:\n",
        "1. **RelevanceEvaluator** (Azure AI) - Is response relevant?\n",
        "2. **TaskAdherenceEvaluator** (Azure AI) - Did agent follow expected workflow?\n",
        "3. **ConcisenessEvaluator** (Custom) - Is response under word limit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fd629d",
      "metadata": {
        "id": "a7fd629d"
      },
      "outputs": [],
      "source": [
        "# Import Azure AI evaluators\n",
        "from azure.ai.evaluation import RelevanceEvaluator, TaskAdherenceEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize evaluators\n",
        "# Note: These use Azure OpenAI by default, but we can configure for GitHub Models\n",
        "\n",
        "# For this workshop, we'll create model config\n",
        "model_config = {\n",
        "    \"type\": \"openai\",\n",
        "    \"api_key\": GITHUB_PAT,\n",
        "    \"model\": \"openai/gpt-4o-mini\",\n",
        "    \"base_url\": \"https://models.github.ai/inference\"\n",
        "}"
      ],
      "metadata": {
        "id": "ECC2Wm0WlTYH"
      },
      "id": "ECC2Wm0WlTYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevance_eval = RelevanceEvaluator(model_config)\n",
        "task_adherence_eval = TaskAdherenceEvaluator(model_config)\n",
        "\n",
        "print(\"âœ… Azure AI evaluators initialized\")"
      ],
      "metadata": {
        "id": "2J8K1RSjlXay"
      },
      "id": "2J8K1RSjlXay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747fccef",
      "metadata": {
        "id": "747fccef"
      },
      "outputs": [],
      "source": [
        "# Custom Conciseness Evaluator (code-based)\n",
        "# Following: https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators#code-based-evaluators\n",
        "\n",
        "class ConcisenessEvaluator:\n",
        "    \"\"\"\n",
        "    Custom code-based evaluator for response conciseness.\n",
        "\n",
        "    Checks if agent response is under a word limit (default: 50 words).\n",
        "    This is a deterministic, rule-based evaluator.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_words: int = 50):\n",
        "        self.max_words = max_words\n",
        "\n",
        "    def __call__(self, *, response: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate conciseness of response.\n",
        "\n",
        "        Args:\n",
        "            response: The agent's response text\n",
        "\n",
        "        Returns:\n",
        "            Dict with 'conciseness' score (0 or 1) and 'conciseness_reason'\n",
        "        \"\"\"\n",
        "        # TODO: Implement word counting logic\n",
        "        # Hint: Use len(response.split())\n",
        "        # Hint: Return score 1 if under limit, 0 if over\n",
        "\n",
        "        return {\"\": \"\"}\n",
        "\n",
        "conciseness_eval = ConcisenessEvaluator(max_words=50)\n",
        "print(\"âœ… Custom Conciseness evaluator created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>Solution</b></summary>\n",
        "  \n",
        "```python\n",
        "def __call__(self, *, response: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate conciseness of response.\n",
        "        \n",
        "        Args:\n",
        "            response: The agent's response text\n",
        "            \n",
        "        Returns:\n",
        "            Dict with 'conciseness' score (0 or 1) and 'reasoning'\n",
        "        \"\"\"        \n",
        "        word_count = len(response.split())\n",
        "        score = 1 if word_count <= self.max_words else 0\n",
        "        \n",
        "        return {\n",
        "            \"conciseness\": score,\n",
        "            \"reasoning\": f\"{word_count} words ({'âœ“' if score == 1 else 'âœ—'} limit: {self.max_words})\"\n",
        "        }\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "NvOpeCsAleFy"
      },
      "id": "NvOpeCsAleFy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Evaluators"
      ],
      "metadata": {
        "id": "9Ce5mbgZmCOC"
      },
      "id": "9Ce5mbgZmCOC"
    },
    {
      "cell_type": "code",
      "source": [
        "test_query = \"What events are happening on campus?\"\n",
        "test_response = \"You've been registered for the AI & Machine Learning Workshop on 2024-03-25 at 10 AM in Seminar Hall A.\""
      ],
      "metadata": {
        "id": "v_msT2p6mAb7"
      },
      "id": "v_msT2p6mAb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Relevance Evaluator\n",
        "print(\"\\n1ï¸âƒ£ RELEVANCE EVALUATOR\")\n",
        "print(\"   Checks if response answers the query\")\n",
        "rel_result = relevance_eval(query=test_query, response=test_response)\n",
        "print(f\"   Score: {rel_result.get('relevance', 0)}\")\n",
        "print(f\"   Reasoning: {rel_result.get('relevance_reason', 'N/A')}\")"
      ],
      "metadata": {
        "id": "M83ETgvumGnq"
      },
      "id": "M83ETgvumGnq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Conciseness Evaluator\n",
        "print(\"\\n2ï¸âƒ£ CONCISENESS EVALUATOR\")\n",
        "print(\"   Checks if response is under word limit\")\n",
        "conc_result = conciseness_eval(response=test_response)\n",
        "print(f\"   Score: {conc_result.get('conciseness', 0)}\")\n",
        "print(f\"   Reasoning: {conc_result.get('conciseness_reason', 'N/A')}\")"
      ],
      "metadata": {
        "id": "xXflX1GjmI_S"
      },
      "id": "xXflX1GjmI_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 3: Task Adherence Evaluator\n",
        "print(\"\\n3ï¸âƒ£ TASK ADHERENCE EVALUATOR\")\n",
        "print(\"   Checks if agent followed expected workflow/used correct tools\")\n",
        "print(\"\\n   âš ï¸ This evaluator needs a TRACE, not just text response!\")\n",
        "print(\"   A trace shows: What tools were called? What were the results?\")\n",
        "print(\"\\n   Let's test with a real agent response...\")\n",
        "\n",
        "# Run agent to get a response with tool calls\n",
        "test_agent_response = await baseline_agent.run(\"What events are happening on campus?\")\n",
        "print_agent_response(test_agent_response, show_details=True)\n",
        "\n",
        "# Convert to trace format using our utility function\n",
        "trace = convert_agent_response_to_trace(test_agent_response)\n",
        "print(f\"\\n   Trace format (what TaskAdherence sees):\")\n",
        "print(json.dumps(trace, indent=2))\n",
        "\n",
        "# Now evaluate with TaskAdherenceEvaluator\n",
        "task_result = task_adherence_eval(\n",
        "    query=\"What events are happening on campus?\",\n",
        "    response=trace,\n",
        ")\n",
        "print(f\"\\n   Score: {task_result.get('task_adherence', 0)}\")\n",
        "print(f\"   Reasoning: {task_result.get('task_adherence_reason', 'N/A')}\")"
      ],
      "metadata": {
        "id": "h511S11CmM2i"
      },
      "id": "h511S11CmM2i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "64b7c119",
      "metadata": {
        "id": "64b7c119"
      },
      "source": [
        "## Part D: Run Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8af588",
      "metadata": {
        "id": "3a8af588"
      },
      "outputs": [],
      "source": [
        "# Helper function to run agent and collect traces\n",
        "async def evaluate_agent_on_dataset(\n",
        "    agent: ChatAgent,\n",
        "    dataset: List[Dict],\n",
        "    label: str = \"Baseline\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Run agent on test dataset and collect evaluation results.\n",
        "    \"\"\"\n",
        "    evaluators = [\"relevance\", \"task_adherence\", \"conciseness\"]\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating {label} Agent on {len(dataset)} test cases\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for i, test_case in enumerate(dataset, 1):\n",
        "        print(f\"[{i}/{len(dataset)}] {test_case['query']}\")\n",
        "\n",
        "        scores = {}\n",
        "        reasoning = {}\n",
        "\n",
        "        # Run agent\n",
        "        response = await agent.run(test_case[\"query\"])\n",
        "        response_text = response.text\n",
        "\n",
        "        # Convert to trace format for TaskAdherence\n",
        "        trace = convert_agent_response_to_trace(response)\n",
        "\n",
        "        # Run evaluators\n",
        "        for evaluator in evaluators:\n",
        "            if evaluator == \"relevance\":\n",
        "                result = relevance_eval(\n",
        "                    query=test_case[\"query\"],\n",
        "                    response=response_text\n",
        "                )\n",
        "            elif evaluator == \"task_adherence\":\n",
        "                result = task_adherence_eval(\n",
        "                    query=test_case[\"query\"],\n",
        "                    response=trace,\n",
        "                )\n",
        "            elif evaluator == \"conciseness\":\n",
        "                result = conciseness_eval(response=response_text)\n",
        "\n",
        "            scores[evaluator] = result.get(f\"{evaluator}\", 0)\n",
        "            reasoning[evaluator] = result.get(f\"{evaluator}_reason\", \"N/A\")\n",
        "\n",
        "        # Collect results\n",
        "        result = {\n",
        "            \"query\": test_case[\"query\"],\n",
        "            \"response\": response_text,\n",
        "            \"scores\": scores,\n",
        "            \"reasoning\": reasoning\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Show progress\n",
        "        print(f\"  Scores: Rel={result['scores']['relevance']:.2f} Task={result['scores']['task_adherence']:.2f} Conc={result['scores']['conciseness']:.0f}\")\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_scores = {\n",
        "        metric: np.mean([r[\"scores\"][metric] for r in results])\n",
        "        for metric in [\"relevance\", \"task_adherence\", \"conciseness\"]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{label} Results:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for metric, score in avg_scores.items():\n",
        "        print(f\"{metric.capitalize():20s}: {score:.2f} (Range: {'1-5' if metric == 'relevance' else '0-1'})\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"avg_scores\": avg_scores,\n",
        "        \"label\": label\n",
        "    }\n",
        "\n",
        "print(\"âœ… Evaluation runner defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0c1912",
      "metadata": {
        "id": "fc0c1912"
      },
      "outputs": [],
      "source": [
        "# Run baseline evaluation\n",
        "baseline_eval_results = await evaluate_agent_on_dataset(\n",
        "    baseline_agent,\n",
        "    test_dataset,\n",
        "    \"Baseline\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Dive: Analyze Results by Metric"
      ],
      "metadata": {
        "id": "vHXlhlRR1P_f"
      },
      "id": "vHXlhlRR1P_f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep-Dive metric analysis\n",
        "def analyze_by_metric(metric_name: str):\n",
        "    \"\"\"\n",
        "    Display detailed breakdown of a specific metric across all test cases.\n",
        "\n",
        "    Args:\n",
        "        metric_name: One of 'relevance', 'task_adherence', or 'conciseness'\n",
        "    \"\"\"\n",
        "    valid_metrics = ['relevance', 'task_adherence', 'conciseness']\n",
        "    if metric_name not in valid_metrics:\n",
        "        print(f\"âŒ Invalid metric. Choose from: {', '.join(valid_metrics)}\")\n",
        "        return\n",
        "\n",
        "    # Display header\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ðŸ“Š METRIC DEEP DIVE: {metric_name.upper().replace('_', ' ')}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Show metric info\n",
        "    metric_info = {\n",
        "        'relevance': {\n",
        "            'description': 'Measures how well the response addresses the query',\n",
        "            'range': '1-5',\n",
        "            'ideal': '5 (highly relevant)'\n",
        "        },\n",
        "        'task_adherence': {\n",
        "            'description': 'Checks if agent followed expected workflow/used correct tools',\n",
        "            'range': '0-1',\n",
        "            'ideal': '1 (perfect adherence)'\n",
        "        },\n",
        "        'conciseness': {\n",
        "            'description': 'Verifies response is under word limit (50 words)',\n",
        "            'range': '0-1',\n",
        "            'ideal': '1 (within limit)'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    info = metric_info[metric_name]\n",
        "    print(f\"\\nðŸ“– Description: {info['description']}\")\n",
        "    print(f\"ðŸ“ Score Range: {info['range']}\")\n",
        "    print(f\"ðŸŽ¯ Ideal Score: {info['ideal']}\")\n",
        "    print(f\"\\n{'â”€' * 80}\\n\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    scores = [r['scores'][metric_name] for r in baseline_eval_results['results']]\n",
        "    avg_score = np.mean(scores)\n",
        "    min_score = min(scores)\n",
        "    max_score = max(scores)\n",
        "\n",
        "    print(f\"ðŸ“ˆ STATISTICS:\")\n",
        "    print(f\"   Average: {avg_score:.2f}\")\n",
        "    print(f\"   Min:     {min_score:.2f}\")\n",
        "    print(f\"   Max:     {max_score:.2f}\")\n",
        "    print(f\"\\n{'â”€' * 80}\\n\")\n",
        "\n",
        "    # Show case-by-case breakdown\n",
        "    print(f\"ðŸ“‹ CASE-BY-CASE BREAKDOWN:\\n\")\n",
        "\n",
        "    for i, (test_case, result) in enumerate(zip(test_dataset, baseline_eval_results['results']), 1):\n",
        "        score = result['scores'][metric_name]\n",
        "        reasoning = result['reasoning'][metric_name]\n",
        "\n",
        "        # Color-code based on score\n",
        "        if metric_name == 'relevance':\n",
        "            emoji = 'ðŸŸ¢' if score >= 4 else 'ðŸŸ¡' if score >= 3 else 'ðŸ”´'\n",
        "        else:\n",
        "            emoji = 'ðŸŸ¢' if score >= 0.8 else 'ðŸŸ¡' if score >= 0.5 else 'ðŸ”´'\n",
        "\n",
        "        print(f\"{emoji} Case {i}: {test_case['category'].upper()}\")\n",
        "        print(f\"   Query: {test_case['query']}\")\n",
        "        print(f\"   Score: {score:.2f} / {info['range'].split('-')[1]}\")\n",
        "        print(f\"   Reasoning: {reasoning}\")\n",
        "        print(f\"   Response Preview: {result['response'][:100]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Example usage - Try different metrics!\n",
        "print(\"Available metrics: 'relevance', 'task_adherence', 'conciseness'\\n\")\n",
        "print(\"Example: analyze_by_metric('relevance')\\n\")"
      ],
      "metadata": {
        "id": "q6QlFuNz1RmZ"
      },
      "id": "q6QlFuNz1RmZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze Relevance metric\n",
        "analyze_by_metric('relevance')"
      ],
      "metadata": {
        "id": "Y8XOAz5y1XXg"
      },
      "id": "Y8XOAz5y1XXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try analyzing other metrics:\n",
        "# analyze_by_metric('task_adherence')\n",
        "# analyze_by_metric('conciseness')"
      ],
      "metadata": {
        "id": "Q6CJLus21lok"
      },
      "id": "Q6CJLus21lok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part E: Improve the Agent"
      ],
      "metadata": {
        "id": "oohGTRh_sBSq"
      },
      "id": "oohGTRh_sBSq"
    },
    {
      "cell_type": "markdown",
      "id": "a56bda06",
      "metadata": {
        "id": "a56bda06"
      },
      "source": [
        "Based on baseline results, we'll:\n",
        "1. **Improve tool responses** - Include event/venue details\n",
        "2. **Better instructions** - Add conciseness and detail requirements\n",
        "3. **Better descriptions** - Help LLM understand when to use tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved tools (detailed responses with emojis)"
      ],
      "metadata": {
        "id": "My-KbGIGsKr_"
      },
      "id": "My-KbGIGsKr_"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement all the improved tool\n",
        "# Hint: Leverage the ones from Lab 1\n",
        "\n",
        "def get_events_v2() -> str:\n",
        "    # TODO: Implement this\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def get_event_details_v2(\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")]\n",
        ") -> str:\n",
        "    # TODO: Implement this\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def register_for_event_v2(\n",
        "    student_id: Annotated[str, Field(description=\"Student ID\")],\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")],\n",
        "    student_name: Annotated[str, Field(description=\"Student name\")]\n",
        ") -> str:\n",
        "    # TODO: Implement this\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def get_event_participants_v2(\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")]\n",
        ") -> str:\n",
        "    # TODO: Implement this\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "rXjVt6hrr6Qy"
      },
      "id": "rXjVt6hrr6Qy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "\n",
        "```py\n",
        "def get_events_v2() -> str:\n",
        "    \"\"\"Retrieve all campus events with detailed formatting.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{BACKEND_URL}/events\")\n",
        "        events = response.json()\n",
        "\n",
        "        if not events:\n",
        "            return \"No events are currently available.\"\n",
        "\n",
        "        result = f\"Found {len(events)} events:\\n\\n\"\n",
        "        for event in events:\n",
        "            result += f\"â€¢ {event['name']} (ID: {event['event_id']})\\n\"\n",
        "            result += f\"  ðŸ“… {event['date']} at {event['time']}\\n\"\n",
        "            result += f\"  ðŸ“ {event['venue']}\\n\"\n",
        "            result += f\"  ðŸ‘¥ {len(event.get('participants', []))}/{event['max_participants']} registered\\n\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def get_event_details_v2(\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")]\n",
        ") -> str:\n",
        "    \"\"\"Get detailed event information with availability.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{BACKEND_URL}/events/{event_id}\")\n",
        "\n",
        "        if response.status_code == 404:\n",
        "            return f\"Event '{event_id}' not found.\"\n",
        "\n",
        "        event = response.json()\n",
        "\n",
        "        result = f\"ðŸ“‹ {event['name']} (ID: {event['event_id']})\\n\\n\"\n",
        "        result += f\"Description: {event['description']}\\n\"\n",
        "        result += f\"ðŸ“… When: {event['date']} at {event['time']}\\n\"\n",
        "        result += f\"ðŸ“ Where: {event['venue']}\\n\"\n",
        "        result += f\"ðŸ‘¥ Capacity: {len(event.get('participants', []))}/{event['max_participants']}\\n\"\n",
        "\n",
        "        if len(event.get('participants', [])) >= event['max_participants']:\n",
        "            result += \"\\nâš ï¸ Event is FULL\"\n",
        "        else:\n",
        "            result += f\"\\nâœ… {event['max_participants'] - len(event.get('participants', []))} spots available\"\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def register_for_event_v2(\n",
        "    student_id: Annotated[str, Field(description=\"Student ID\")],\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")],\n",
        "    student_name: Annotated[str, Field(description=\"Student name\")]\n",
        ") -> str:\n",
        "    \"\"\"Register student with detailed confirmation.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{BACKEND_URL}/events/{event_id}/register\",\n",
        "            json={\"student_id\": student_id, \"student_name\": student_name}\n",
        "        )\n",
        "        result = response.json()\n",
        "\n",
        "        if result.get(\"success\"):\n",
        "            details = result.get(\"event_details\", {})\n",
        "            confirmation = f\"âœ… Successfully registered {student_name}!\\n\\n\"\n",
        "            confirmation += f\"ðŸ“… {details.get('date', 'TBD')}\\n\"\n",
        "            confirmation += f\"â° {details.get('time', 'TBD')}\\n\"\n",
        "            confirmation += f\"ðŸ“ {details.get('venue', 'TBD')}\"\n",
        "            return confirmation\n",
        "        else:\n",
        "            return f\"âŒ Failed: {result.get('message')}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def get_event_participants_v2(\n",
        "    event_id: Annotated[str, Field(description=\"Event ID\")]\n",
        ") -> str:\n",
        "    \"\"\"Get participant list with count.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{BACKEND_URL}/events/{event_id}/participants\")\n",
        "\n",
        "        if response.status_code == 404:\n",
        "            return \"Event not found.\"\n",
        "\n",
        "        data = response.json()\n",
        "        event_name = data.get('event_name', 'Event')\n",
        "        count = data.get('participant_count', 0)\n",
        "        participants = data.get('participants', [])\n",
        "\n",
        "        if count == 0:\n",
        "            return f\"No registrations for {event_name} yet.\"\n",
        "\n",
        "        result = f\"ðŸ“Š {event_name} - {count} participants\\n\\n\"\n",
        "        for i, sid in enumerate(participants, 1):\n",
        "            result += f\"{i}. {sid}\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "7GpXVezB3KfD"
      },
      "id": "7GpXVezB3KfD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved agent instructions"
      ],
      "metadata": {
        "id": "UW1ns0UmsTbE"
      },
      "id": "UW1ns0UmsTbE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a47102",
      "metadata": {
        "id": "83a47102"
      },
      "outputs": [],
      "source": [
        "# TODO: Write the improved agent instructions\n",
        "# Hint: Use the ones from Lab 1\n",
        "\n",
        "IMPROVED_INSTRUCTIONS = \"\"\"\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Solution</b></summary>\n",
        "    \n",
        "```py\n",
        "IMPROVED_INSTRUCTIONS = \"\"\"\n",
        "You are a helpful campus event assistant.\n",
        "\n",
        "Your capabilities:\n",
        "- Browse all campus events\n",
        "- Get detailed information about specific events\n",
        "- Register students for events\n",
        "- View event participants\n",
        "\n",
        "Guidelines:\n",
        "- Be concise (under 50 words)\n",
        "- ALWAYS include specific details from tool results (event names, dates, venues)\n",
        "- Use emojis when available in tool responses\n",
        "- Confirm successful actions with details\n",
        "- Only call tools when explicitly requested\n",
        "\"\"\"\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "W-0734Kv16Kl"
      },
      "id": "W-0734Kv16Kl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved Agent"
      ],
      "metadata": {
        "id": "pStqnItmsZRW"
      },
      "id": "pStqnItmsZRW"
    },
    {
      "cell_type": "code",
      "source": [
        "improved_agent = ChatAgent(\n",
        "    chat_client=chat_client,\n",
        "    instructions=IMPROVED_INSTRUCTIONS,\n",
        "    tools=[get_events_v2, get_event_details_v2, register_for_event_v2, get_event_participants_v2]\n",
        ")\n",
        "print(\"âœ… Improved agent created\")"
      ],
      "metadata": {
        "id": "-LVFbbHnsXuW"
      },
      "id": "-LVFbbHnsXuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9b69616c",
      "metadata": {
        "id": "9b69616c"
      },
      "source": [
        "## Part F: Re-evaluate & Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d146648a",
      "metadata": {
        "id": "d146648a"
      },
      "outputs": [],
      "source": [
        "# Run improved evaluation\n",
        "improved_eval_results = await evaluate_agent_on_dataset(\n",
        "    improved_agent,\n",
        "    test_dataset,\n",
        "    \"Improved\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb84e271",
      "metadata": {
        "id": "eb84e271"
      },
      "outputs": [],
      "source": [
        "# Visualize comparison (normalized to 0-1 scale)\n",
        "metrics = list(baseline_eval_results[\"avg_scores\"].keys())\n",
        "\n",
        "# Normalize scores: Relevance is 1-5, others are 0-1\n",
        "def normalize_score(metric, score):\n",
        "    if metric == \"relevance\":\n",
        "        return (score - 1) / 4  # Convert 1-5 to 0-1\n",
        "    return score\n",
        "\n",
        "baseline_scores = [normalize_score(m, baseline_eval_results[\"avg_scores\"][m]) for m in metrics]\n",
        "improved_scores = [normalize_score(m, improved_eval_results[\"avg_scores\"][m]) for m in metrics]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline', color='#ff6b6b')\n",
        "rects2 = ax.bar(x + width/2, improved_scores, width, label='Improved', color='#51cf66')\n",
        "\n",
        "ax.set_ylabel('Normalized Score (0-1)')\n",
        "ax.set_title('Agent Performance: Baseline vs Improved')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels showing normalized scores\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "# Add note about normalization\n",
        "ax.text(0.02, 0.98, 'Note: Relevance (1-5) normalized to 0-1 scale',\n",
        "        transform=ax.transAxes, fontsize=8, verticalalignment='top',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8137c8ae",
      "metadata": {
        "id": "8137c8ae"
      },
      "source": [
        "## Part G: Analyze Specific Cases"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a deep dive into specific test cases to understand how improvements translate to real responses."
      ],
      "metadata": {
        "id": "CyztuLywv-uR"
      },
      "id": "CyztuLywv-uR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288e8805",
      "metadata": {
        "id": "288e8805"
      },
      "outputs": [],
      "source": [
        "# Comprehensive case-by-case comparison\n",
        "def analyze_case(idx: int):\n",
        "    \"\"\"Display detailed comparison of baseline vs improved for a specific test case.\"\"\"\n",
        "    test_case = test_dataset[idx]\n",
        "    baseline_result = baseline_eval_results[\"results\"][idx]\n",
        "    improved_result = improved_eval_results[\"results\"][idx]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ðŸ“‹ CASE {idx + 1}: {test_case['category'].upper()}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nâ“ Query: {test_case['query']}\")\n",
        "    print(f\"ðŸŽ¯ Expected: {test_case['expected_outcome']}\\n\")\n",
        "\n",
        "    # Baseline Response\n",
        "    print(\"â”€\" * 80)\n",
        "    print(\"ðŸ”´ BASELINE AGENT\")\n",
        "    print(\"â”€\" * 80)\n",
        "    print(f\"\\nðŸ’¬ Response:\\n{baseline_result['response']}\\n\")\n",
        "\n",
        "    print(\"ðŸ“Š Scores:\")\n",
        "    for metric, score in baseline_result['scores'].items():\n",
        "        if metric == \"relevance\":\n",
        "            print(f\"  â€¢ {metric.replace('_', ' ').title():18s}: {score:.2f} / 5.00\")\n",
        "        else:\n",
        "            print(f\"  â€¢ {metric.replace('_', ' ').title():18s}: {score:.2f} / 1.00\")\n",
        "\n",
        "    print(\"\\nðŸ“ Rationales:\")\n",
        "    for metric, reasoning in baseline_result['reasoning'].items():\n",
        "        print(f\"  â€¢ {metric.replace('_', ' ').title()}:\")\n",
        "        # Truncate long rationales\n",
        "        reasoning_text = reasoning\n",
        "        print(f\"    {reasoning_text}\")\n",
        "\n",
        "    # Improved Response\n",
        "    print(\"\\n\" + \"â”€\" * 80)\n",
        "    print(\"ðŸŸ¢ IMPROVED AGENT\")\n",
        "    print(\"â”€\" * 80)\n",
        "    print(f\"\\nðŸ’¬ Response:\\n{improved_result['response']}\\n\")\n",
        "\n",
        "    print(\"ðŸ“Š Scores:\")\n",
        "    for metric, score in improved_result['scores'].items():\n",
        "        if metric == \"relevance\":\n",
        "            print(f\"  â€¢ {metric.replace('_', ' ').title():18s}: {score:.2f} / 5.00\")\n",
        "        else:\n",
        "            print(f\"  â€¢ {metric.replace('_', ' ').title():18s}: {score:.2f} / 1.00\")\n",
        "\n",
        "    print(\"\\nðŸ“ Rationales:\")\n",
        "    for metric, reasoning in improved_result['reasoning'].items():\n",
        "        print(f\"  â€¢ {metric.replace('_', ' ').title()}:\")\n",
        "        reasoning_text = reasoning\n",
        "        print(f\"    {reasoning_text}\")\n",
        "\n",
        "    # Score Improvements\n",
        "    print(\"\\n\" + \"â”€\" * 80)\n",
        "    print(\"ðŸ“ˆ IMPROVEMENTS\")\n",
        "    print(\"â”€\" * 80)\n",
        "    for metric in baseline_result['scores'].keys():\n",
        "        baseline_score = baseline_result['scores'][metric]\n",
        "        improved_score = improved_result['scores'][metric]\n",
        "\n",
        "        if baseline_score > 0:\n",
        "            change = ((improved_score - baseline_score) / baseline_score) * 100\n",
        "            change_str = f\"{change:+.1f}%\"\n",
        "        else:\n",
        "            change_str = \"N/A\"\n",
        "\n",
        "        # Show improvement with context\n",
        "        if metric == \"relevance\":\n",
        "            print(f\"  â€¢ {metric.replace('_', ' ').title():18s}: {baseline_score:.2f} â†’ {improved_score:.2f} ({change_str})\")\n",
        "        else:\n",
        "            print(f\"  â€¢ {metric.replace('_', ' ').title():18s}: {baseline_score:.2f} â†’ {improved_score:.2f} ({change_str})\")\n",
        "\n",
        "    print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze first case\n",
        "analyze_case(0)"
      ],
      "metadata": {
        "id": "BCfO-vgPwTCs"
      },
      "id": "BCfO-vgPwTCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze another interesting case (e.g., registration if available)\n",
        "if len(test_dataset) > 2:\n",
        "    analyze_case(2)"
      ],
      "metadata": {
        "id": "KADV-XkPwsIt"
      },
      "id": "KADV-XkPwsIt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ‰ Lab 2 Complete!"
      ],
      "metadata": {
        "id": "JJrwaimOxE5D"
      },
      "id": "JJrwaimOxE5D"
    },
    {
      "cell_type": "markdown",
      "id": "2545797a",
      "metadata": {
        "id": "2545797a"
      },
      "source": [
        "### What You Accomplished:\n",
        "\n",
        "âœ… **Created test dataset** with diverse event-focused cases  \n",
        "âœ… **Used Azure AI evaluators** (Relevance, Task Adherence)  \n",
        "âœ… **Built custom evaluator** (Conciseness)  \n",
        "âœ… **Converted traces** for TaskAdherenceEvaluator  \n",
        "âœ… **Ran baseline evaluation** and identified issues  \n",
        "âœ… **Improved agent systematically** with detailed responses  \n",
        "âœ… **Measured improvement** quantitatively\n",
        "\n",
        "### Key Learnings:\n",
        "\n",
        "1. **Azure AI Evaluation SDK** - Production-ready evaluators\n",
        "2. **Custom evaluators** - Code-based for deterministic rules\n",
        "3. **Trace conversion** - Enable advanced evaluators like TaskAdherence\n",
        "4. **Systematic improvement** - Measure â†’ Improve â†’ Re-measure\n",
        "5. **Quantifiable impact** - Metrics enable objective decisions\n",
        "\n",
        "### Production Best Practices:\n",
        "\n",
        "1. Expand test dataset to 50-100 cases per capability\n",
        "2. Add more evaluators (Groundedness, Safety, Fluency)\n",
        "3. Automate evaluation in CI/CD pipeline\n",
        "4. Monitor production metrics continuously\n",
        "5. A/B test improvements before deployment\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Extension Challenge (Optional)"
      ],
      "metadata": {
        "id": "eH4QbABmxM6_"
      },
      "id": "eH4QbABmxM6_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want to practice what you've learned? Try evaluating agents for the **venue booking** and **notification** scenarios from the mock backend!\n",
        "\n"
      ],
      "metadata": {
        "id": "iqFT-Vt3xTb2"
      },
      "id": "iqFT-Vt3xTb2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 1: Venue Booking Agent\n",
        "\n",
        "**Tools to implement:**\n",
        "- `get_venues()` - List available venues\n",
        "- `get_venue_availability(venue_id, date)` - Check availability\n",
        "- `book_venue(venue_id, club_name, date, time_slot, purpose, attendees)` - Book venue\n",
        "\n",
        "**Test dataset ideas:**\n",
        "- Browse all venues\n",
        "- Check specific venue availability\n",
        "- Book venue with all details\n",
        "- Handle booking conflicts\n",
        "- Multi-step: browse â†’ check â†’ book"
      ],
      "metadata": {
        "id": "QFvsFExRxW7j"
      },
      "id": "QFvsFExRxW7j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools to implement:**\n",
        "- `get_venues()` - List available venues\n",
        "- `get_venue_availability(venue_id, date)` - Check availability\n",
        "- `book_venue(venue_id, club_name, date, time_slot, purpose, attendees)` - Book venue\n",
        "\n",
        "**Test dataset ideas:**\n",
        "- Browse all venues\n",
        "- Check specific venue availability\n",
        "- Book venue with all details\n",
        "- Handle booking conflicts\n",
        "- Multi-step: browse â†’ check â†’ book"
      ],
      "metadata": {
        "id": "VjcbAXFYxZym"
      },
      "id": "VjcbAXFYxZym"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 2: Notification Agent"
      ],
      "metadata": {
        "id": "VUyDC6nOxcfX"
      },
      "id": "VUyDC6nOxcfX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools to implement:**\n",
        "- `send_event_notification(event_id, message)` - Send to event participants\n",
        "- (Could extend with: list notification history, send targeted notifications)\n",
        "\n",
        "**Test dataset ideas:**\n",
        "- Send event reminders\n",
        "- Send venue change notifications\n",
        "- Send cancellation notices\n",
        "- Handle invalid event IDs"
      ],
      "metadata": {
        "id": "nALr0MiWxdxR"
      },
      "id": "nALr0MiWxdxR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 3: Unified Campus Agent"
      ],
      "metadata": {
        "id": "GFlQU_ZFx1vu"
      },
      "id": "GFlQU_ZFx1vu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine all three domains (events, venues, notifications) into one agent:\n",
        "- Import tools from Labs 1 & 2\n",
        "- Create unified instructions covering all capabilities\n",
        "- Build comprehensive test dataset (20+ cases)\n",
        "- Evaluate cross-domain query handling\n",
        "\n",
        "**Example queries:**\n",
        "- \"Register me for AI Workshop and book Seminar Hall B for our club meeting\"\n",
        "- \"Show me events, then notify TechFest participants about the venue change\""
      ],
      "metadata": {
        "id": "xCPl7Dpcx2kV"
      },
      "id": "xCPl7Dpcx2kV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Tips:"
      ],
      "metadata": {
        "id": "DYZpWyBuyD3W"
      },
      "id": "DYZpWyBuyD3W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reuse the evaluation framework from this lab\n",
        "- Consider domain-specific evaluators (e.g., \"ParameterExtraction\" for venue booking)\n",
        "- Test edge cases: conflicts, missing data, invalid IDs\n",
        "- Measure improvement iterations\n",
        "\n",
        "**Happy evaluating! ðŸŽ¯**"
      ],
      "metadata": {
        "id": "xM8kzu6eyF4J"
      },
      "id": "xM8kzu6eyF4J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Congratulations on completing the workshop! ðŸš€**"
      ],
      "metadata": {
        "id": "97Lm2aWoyLRp"
      },
      "id": "97Lm2aWoyLRp"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}